# -*- coding: utf-8 -*-
"""HGFACE Bootcmp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_7okt_7qbU70C04fqwJoIJ_ZOvfoSwiT
"""

!nvidia-smi

"""## 1. *TEXT GENERATION MODEL(MODEL- GPT2)*"""

from transformers import pipeline
generator = pipeline("text-generation", model='gpt2')
response = generator('what is the Hugging face',max_length = 40 ,num_return_sequences=1)
print(response[0]['generated_text'])

"""## 2. *NER (NAMED ENTITY RECOGNITION)*"""

from transformers import pipeline
ner = pipeline("ner",grouped_entities=True)
entities = ner("Hugging face is based on NYC and partner with google with $200 dollar price")
print(entities)

"""## 3. *SENTIMENT ANALYSIS*"""

!pip install transformers Datasets

from transformers import pipeline
classifier = pipeline("sentiment-analysis")
result = classifier("I love Huging face library vary much, it is so nice")
print(result)

from transformers import pipeline
classifier = pipeline("sentiment-analysis")
result = classifier("the food is not tasty")
print(result)

!pip install gradio

import gradio as gr
classifier = pipeline("sentiment-analysis")

def analyze_sentiment(text):
  return classifier(text)[0]['label']

gr.Interface(fn=analyze_sentiment, inputs="text", outputs="text").launch()

"""## 4.*TEXT TO IMAGE MODEL USING DIFFUSION PIPELINE*"""

from diffusers import StableDiffusionPipeline
import torch

model_id = "sd-legacy/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "beautyfull girl walking in sttreet light with her pet dog "
image = pipe(prompt).images[0]

image.save("beautyfull girl.png")

import gradio as gr
from diffusers import StableDiffusionPipeline
import torch

# load model (fp16 + CUDA for perfomance)
model_id = "sd-legacy/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,use_auth_token=True)
pipe = pipe.to("cuda")

def generated_image(prompt):
  image=pipe(prompt).images[0]
  image.save(f"outputs/{prompt}.png")
  return image

#UI
gr.Interface(
    fn=generated_image,
    inputs=gr.Textbox(label='Enter your prompt', placeholder="e.g. a beautyfull girl walking in sttreet light with her pet dog "),
    outputs=gr.Image(type='pil',label='Generated Image'),
    title='Ashok text-Image Generator',
    description ='Enter your prompt and see your generated image'
).launch()

"""## *CHAT GPT CODE*


"""

import gradio as gr
from diffusers import StableDiffusionPipeline
import torch
import os

# Ensure outputs folder exists
os.makedirs("outputs", exist_ok=True)

# Load model (fp16 + CUDA for performance)
model_id = "sd-legacy/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

# Image generation function
def generated_image(prompt):
    image = pipe(prompt).images[0]
    safe_filename = "".join(c for c in prompt if c.isalnum() or c in (" ", "_")).rstrip()
    image.save(f"outputs/{safe_filename}.png")
    return image

# UI
gr.Interface(
    fn=generated_image,
    inputs=gr.Textbox(label='Enter your prompt', placeholder="e.g. a beautiful girl walking in street light with her pet dog"),
    outputs=gr.Image(type='pil', label='Generated Image'),
    title='Ashok Text-to-Image Generator',
    description='Enter your prompt and see your generated image'
).launch()